{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**코스피의 10일 가격 데이터를 상태로 사용하고, 두 ETF 상품(인버스, 레버리지)의 비율을 행동으로 하는 강화학습 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal, Beta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **모델 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **메모리 버퍼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **에이전트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std, NN_conf, use_gpu = True):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # action mean range 0 to 1\n",
    "        # action dim = 1 \n",
    "        if NN_conf == 'sigmoid':\n",
    "            self.actor =  nn.Sequential(\n",
    "                    nn.Linear(state_dim, 128),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(64, action_dim),\n",
    "                    nn.Sigmoid()\n",
    "                    )\n",
    "            # critic\n",
    "            self.critic = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 128),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(64, 1)\n",
    "                    )\n",
    "        elif NN_conf == 'relu':\n",
    "            print('ReLU')\n",
    "            self.actor =  nn.Sequential(\n",
    "                    nn.Linear(state_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, action_dim),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "            # critic\n",
    "            self.critic = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 1)\n",
    "                    )\n",
    "            \n",
    "        self.set_device(use_gpu)\n",
    "\n",
    "        self.action_var = torch.full((action_dim,), action_std*action_std).to(self.device)\n",
    "\n",
    "    def set_device(self, use_gpu = False):\n",
    "        if use_gpu:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state, memory, gready):\n",
    "        action_mean = self.actor(state)\n",
    "        if not gready:\n",
    "            cov_mat = torch.diag(self.action_var).to(self.device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action, 0, 1)  # Clamping the action\n",
    "            \n",
    "            action_logprob = dist.log_prob(action)\n",
    "            \n",
    "            memory.states.append(state)\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(action_logprob)\n",
    "                 \n",
    "            return action.detach()\n",
    "        \n",
    "        else:\n",
    "            return action_mean.detach()\n",
    "\n",
    "    def evaluate(self, state, action):   \n",
    "        # action mean, get the 1 actions from NN\n",
    "        action_mean = self.actor(state)\n",
    "        #print(action_mean)\n",
    "        # [0.25, 0.25 , 0.25]. Expand this tesor to same size as other\n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        # creates diag matrix 3x3\n",
    "        #print(self.device)\n",
    "        cov_mat = torch.diag_embed(action_var).to(self.device)\n",
    "        \n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        \n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "        #print('state', state_value)\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, conf_ppo, use_gpu=False):\n",
    "        self.lr = conf_ppo['lr']\n",
    "        self.betas = conf_ppo['betas']\n",
    "        self.gamma = conf_ppo['gamma']\n",
    "        self.eps_clip = conf_ppo['eps_clip']\n",
    "        self.K_epochs = conf_ppo['K_epochs']\n",
    "        action_std = conf_ppo['action_std']\n",
    "        self.set_device(use_gpu)\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std, NN_conf=conf_ppo['nn_type'], use_gpu=use_gpu).to(self.device)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std, NN_conf=conf_ppo['nn_type'], use_gpu=use_gpu).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr, betas=self.betas)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.lam_a = conf_ppo['lam_a']\n",
    "        self.normalize_rewards = conf_ppo['normalize_rewards']\n",
    "        self.loss_total = 0.0\n",
    "        self.loss_a = 0.0\n",
    "        self.loss_max = 0.0\n",
    "        self.loss_min = 0.0\n",
    "\n",
    "    def set_device(self, use_gpu=True, set_policy=False):\n",
    "        if use_gpu:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "        if set_policy:\n",
    "            self.policy.actor.to(self.device)\n",
    "            self.policy.critic.to(self.device)\n",
    "            self.policy.action_var.to(self.device)\n",
    "            self.policy.set_device(self.device)\n",
    "            self.policy_old.actor.to(self.device)\n",
    "            self.policy_old.critic.to(self.device)\n",
    "            self.policy_old.action_var.to(self.device)\n",
    "            self.policy_old.set_device(self.device)\n",
    "\n",
    "    def select_action(self, state, memory, greedy=False):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.policy_old.act(state, memory, greedy).cpu().data.numpy().flatten()\n",
    "\n",
    "    def estimate_action(self, state, action):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        action = torch.FloatTensor(action.reshape(1, -1)).to(self.device)\n",
    "        return self.policy_old.evaluate(state, action)\n",
    "\n",
    "    def update(self, memory, to_tensor=False, use_gpu=True):\n",
    "        self.set_device(use_gpu, set_policy=True)\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        if to_tensor:\n",
    "            memory.states = [torch.FloatTensor(i.reshape(1, -1)).to(self.device) for i in memory.states]\n",
    "            memory.actions = [torch.FloatTensor(i.reshape(1, -1)).to(self.device) for i in memory.actions]\n",
    "            memory.logprobs = [torch.FloatTensor(i.reshape(1, -1)).to(self.device) for i in memory.logprobs]\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        if self.normalize_rewards:\n",
    "            rewards = ((rewards - rewards.mean())/(rewards.std() + 1e-7)).to(self.device)\n",
    "        old_states = torch.squeeze(torch.stack(memory.states).to(self.device), 1).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).to(self.device), 1).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).to(self.device).detach()\n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            mseLoss = 0.5 * self.MseLoss(state_values, rewards)\n",
    "            loss = -torch.min(surr1, surr2) + mseLoss - 0.01 * dist_entropy\n",
    "            if self.lam_a != 0:\n",
    "                mu = torch.squeeze(torch.stack(memory.actions[:-1]).to(self.device), 1).detach()\n",
    "                mu_nxt = torch.squeeze(torch.stack(memory.actions[1:]).to(self.device), 1).detach()\n",
    "                loss += 0.5 * self.MseLoss(mu_nxt, mu) * self.lam_a\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self.loss_total = loss.cpu().data.numpy().flatten()[0]\n",
    "        self.loss_a = mseLoss.cpu().data.numpy().flatten()[0]\n",
    "        self.loss_max = advantages.max().cpu().data.numpy().flatten()[0]\n",
    "        self.loss_min = advantages.min().cpu().data.numpy().flatten()[0]\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **환경 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, config, scaler):\n",
    "        self.config = config\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        self.kospi_data = pd.read_csv(self.config['kospi_path']).iloc[:, 1:].values\n",
    "        self.leverage_data = pd.read_csv(self.config['leverage_path'])['Close'].values\n",
    "        self.inverse_data = pd.read_csv(self.config['inverse_path'])['Close'].values\n",
    "        \n",
    "    def reset(self, rand_state=False):\n",
    "        self.day = 0\n",
    "        self.reset_reward()\n",
    "        self.init_cash = self.config['initial_cash']\n",
    "        \n",
    "        self.price_a = self.leverage_data[self.day]\n",
    "        self.price_b = self.inverse_data[self.day]\n",
    "        if rand_state:\n",
    "            random_ratio = np.random.rand()\n",
    "            self.na = int(self.init_cash * random_ratio / self.price_a)\n",
    "            self.nb = int(self.init_cash * (1-random_ratio) / self.price_b)\n",
    "        else:\n",
    "            self.na = int(self.init_cash * 0.5 / self.price_a)\n",
    "            self.nb = int(self.init_cash * 0.5 / self.price_b)\n",
    "        self.init_total = self.na * self.price_a + self.nb * self.price_b\n",
    "        self.total_cash = self.init_total\n",
    "        # print(f\"reset -> day : {self.day}, {self.total_cash} = {self.na} * {self.price_a} + {self.nb} * {self.price_b}\")\n",
    "        self.state = self.get_state()\n",
    "        # print(f\"init state : {self.state}\")\n",
    "        return self.state\n",
    "    \n",
    "    def get_state(self):\n",
    "        data = self.kospi_data[self.day:self.day + self.config['state_window']]\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return scaled_data.flatten()\n",
    "    \n",
    "    def get_reward(self):\n",
    "        self.price_a = self.leverage_data[self.day]\n",
    "        self.price_b = self.inverse_data[self.day]\n",
    "        self.total_cash = self.na * self.price_a + self.nb * self.price_b\n",
    "        reward = self.total_cash - self.init_total\n",
    "        # print(f\"{self.total_cash} = {self.na} * {self.price_a} + {self.nb} * {self.price_b}\")\n",
    "        return reward\n",
    "    \n",
    "    def reset_reward(self):\n",
    "        self.reward_max_time = 0\n",
    "        self.reward_trigger = 0\n",
    "        \n",
    "    def is_done(self, reward):\n",
    "        if self.day == self.config['sim_day_max']:\n",
    "            print(\"Max Time\")\n",
    "            self.reward_max_time = 1\n",
    "            return True\n",
    "        \n",
    "        if reward / self.init_total < -0.05:\n",
    "            # print(\"Stop loss triggered\")\n",
    "            self.reward_trigger = 1\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "            \n",
    "    def step(self, action):\n",
    "        leverage_ratio = action[0]\n",
    "        inverse_ratio = 1 - leverage_ratio\n",
    "        \n",
    "        self.na = int(self.total_cash * leverage_ratio / self.price_a)\n",
    "        self.nb = int(self.total_cash * inverse_ratio / self.price_b)\n",
    "        # print(f\"step -> day : {self.day}, {self.na} = int({self.total_cash} * {leverage_ratio} / {self.price_a}), {self.nb} = int({self.total_cash} * {inverse_ratio} / {self.price_b})\")\n",
    "        \n",
    "        self.day += 1\n",
    "        reward = self.get_reward()\n",
    "        done = self.is_done(reward)\n",
    "        state = self.get_state()\n",
    "        # print(f\"step -> day : {self.day}, state : {state}, reward : {reward}, done : {done}\")\n",
    "        return state, reward, done, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kospi_data = pd.read_csv(conf['kospi_path']).iloc[:, 1:].values\n",
    "# kospi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_reward = 0.0\n",
    "# state_dim = conf['state_dim']\n",
    "# action_dim = conf['action_dim']\n",
    "# memory = Memory()\n",
    "\n",
    "# ppo = PPO(state_dim, action_dim, conf)\n",
    "# env = TradingEnv(conf)\n",
    "# state = env.reset(rand_state=False)\n",
    "\n",
    "# done = False\n",
    "# action = np.zeros(1)\n",
    "# for i in range(30):\n",
    "#     act = ppo.select_action(state, memory)\n",
    "#     action[0] = act[0]\n",
    "#     print(f\"action : {action}\")\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     print(f\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **에피소드 진행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(kospi_data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(kospi_data)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    writer = SummaryWriter('runs/'+ config['track'])\n",
    "    memory = Memory()\n",
    "    state_dim = config['state_dim']\n",
    "    action_dim = config['action_dim']\n",
    "    \n",
    "    ppo = PPO(state_dim, action_dim, config)\n",
    "    \n",
    "    best_reward = -float('inf')\n",
    "    time_step = 0\n",
    "    \n",
    "    kospi_data = pd.read_csv(config['kospi_path']).iloc[:, 1:].values\n",
    "    scaler = get_scaler(kospi_data)\n",
    "    env = TradingEnv(config, scaler)\n",
    "    \n",
    "    for i_episode in tqdm(range(1, config['max_episodes']+1)):\n",
    "        \n",
    "        running_reward = 0.0\n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        action = np.zeros(1)\n",
    "        step_day = 0\n",
    "        \n",
    "        while not done:\n",
    "            act = ppo.select_action(state, memory)\n",
    "            action[0] = act[0]\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Log step reward and action\n",
    "            writer.add_scalar(f\"Episode_{i_episode}/Step_Reward\", running_reward, step_day)\n",
    "            writer.add_scalar(f\"Episode_{i_episode}/Step_Action\", action[0], step_day)\n",
    "            \n",
    "            # store rewards\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            step_day += 1\n",
    "            time_step += 1\n",
    "            running_reward += reward\n",
    "            ## PPO update\n",
    "            if time_step % config['update_timestep'] == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                time_step = 0\n",
    "                \n",
    "        writer.add_scalar(\"Reward\", running_reward, i_episode)\n",
    "        writer.add_scalar(\"Loss\", ppo.loss_total, i_episode)\n",
    "        writer.add_scalar(\"Loss_MSE\", ppo.loss_a, i_episode)\n",
    "        writer.add_scalar(\"Loss_adv_max\", ppo.loss_max, i_episode)\n",
    "        writer.add_scalar(\"Loss_adv_min\", ppo.loss_min, i_episode)\n",
    "        writer.add_scalar(\"Max_Time\", env.reward_max_time, i_episode)\n",
    "        writer.add_scalar(\"Trigger\", env.reward_trigger, i_episode)\n",
    "        \n",
    "        # save best model\n",
    "        if running_reward > best_reward:\n",
    "            best_reward = running_reward\n",
    "            torch.save(ppo.policy.state_dict(), f'ppo_models/best_stock_ppo.pth')\n",
    "        \n",
    "    # save model after training\n",
    "    torch.save(ppo.policy.state_dict(), 'ppo_models/stock_ppo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'track': 'RL_ppo_training',\n",
    "    'max_episodes': 1000,\n",
    "    'update_timestep': 200,\n",
    "    'action_std': 0.6,\n",
    "    'K_epochs': 80, # update policy for K epochs\n",
    "    'eps_clip': 0.2, # clip parameter for PPO\n",
    "    'gamma': 1.0, # discount factor\n",
    "    'lr': 1e-5,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'random_seed': None,\n",
    "    'lam_a': 0,\n",
    "    'normalize_rewards': True,\n",
    "    'nn_type': 'sigmoid',\n",
    "    'state_dim': 60, # 10일 X 6개 특징\n",
    "    'action_dim': 1,\n",
    "    'sim_day_max': 30,\n",
    "    'kospi_path': 'data/RL/kospi_rl.csv',\n",
    "    'leverage_path': 'data/RL/leverage_rl.csv',\n",
    "    'inverse_path': 'data/RL/inverse_rl.csv',\n",
    "    'lstm_path': 'lstm_models/best_lstm_stock_1e-3.pth',\n",
    "    'initial_cash': 1000000,\n",
    "    'state_window': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 158/1000 [00:09<00:35, 23.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 172/1000 [00:10<00:53, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 195/1000 [00:12<00:49, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 217/1000 [00:14<01:04, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 263/1000 [00:19<01:41,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 317/1000 [00:26<01:51,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 419/1000 [00:39<01:11,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 424/1000 [00:41<01:49,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 451/1000 [00:43<00:54,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 453/1000 [00:45<02:29,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 456/1000 [00:45<01:54,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 460/1000 [00:46<01:20,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 474/1000 [00:48<00:58,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 486/1000 [00:50<00:50, 10.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 496/1000 [00:53<01:30,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 513/1000 [00:55<00:56,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 553/1000 [01:01<01:18,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 588/1000 [01:05<00:36, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 595/1000 [01:06<00:52,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [01:06<00:36, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 614/1000 [01:08<00:31, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 621/1000 [01:09<00:44,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 636/1000 [01:10<00:36, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 656/1000 [01:13<00:50,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 693/1000 [01:16<00:25, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 696/1000 [01:16<00:23, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 698/1000 [01:17<00:50,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 715/1000 [01:19<00:25, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 722/1000 [01:20<00:41,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 728/1000 [01:21<00:24, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 741/1000 [01:22<00:24, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 772/1000 [01:25<00:18, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 777/1000 [01:26<00:30,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 781/1000 [01:27<00:23,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 783/1000 [01:27<00:21, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 811/1000 [01:30<00:14, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 820/1000 [01:31<00:17, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 838/1000 [01:33<00:12, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 852/1000 [01:34<00:12, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 868/1000 [01:36<00:10, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 895/1000 [01:39<00:07, 14.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [01:40<00:12,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 908/1000 [01:40<00:06, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 914/1000 [01:41<00:10,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 921/1000 [01:41<00:06, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 928/1000 [01:43<00:08,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 943/1000 [01:44<00:06,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 956/1000 [01:46<00:04,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 959/1000 [01:46<00:03, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 964/1000 [01:47<00:04,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 984/1000 [01:49<00:01, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time\n",
      "Max Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:50<00:00,  9.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
